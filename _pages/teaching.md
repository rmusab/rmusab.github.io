---
layout: page
permalink: /teaching/
title: Teaching
description: Explore my collection of teaching materials, resources, methods, and tips gathered from my university studies, as well as teaching and work experience. Learn and grow with valuable insights and tools
nav: true
nav_order: 5
---

## My courses

You can find a list of the courses that I taught at the University of Washington [here](https://math.washington.edu/people/ravil-mussabayev).

---

## Resources

In this section, I have curated resources that have been instrumental in my research. I hope they prove valuable for those delving into these subjects.

### Natural Language Processing (NLP)

- A wonderful [introductory course on NLP](https://lena-voita.github.io/nlp_course.html) from [Lena Voita](https://lena-voita.github.io/). It covers basic topics in a very beginner friendly and visual format. Moreover, each chapter contains research-oriented questions that can inspire a reader to think about the learned material in novel ways;

- A nicely illustrated blog post about [transformers](https://jalammar.github.io/illustrated-transformer/) and [Seq2Seq models with attention](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/) from [Jay Alammar](https://jalammar.github.io/). These posts can serve as a great entry point into the nuts and bolts of transformer-based models. His blog also contains some other well-visualized posts on machine learning topics;

- A [video course on NLP from Standford University](https://youtu.be/8rXD5-xhemo) (CS224N, Winter 2019). It can be used as a good theoretical introduction into the basics of NLP;

- [A visual introduction to information theory](https://colah.github.io/posts/2015-09-Visual-Information/). This post covers the basic notions of information theory, like optimal encoding, entropy, cross-entropy, mutual information and other concepts that are vital to how NLP models learn from data.
